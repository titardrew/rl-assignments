{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58f8d078d68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from lib import get_network\n",
    "from lib.agent import DQN\n",
    "from lib.logging import Logger\n",
    "from lib.rollout import ReplayBuffer\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading https://files.pythonhosted.org/packages/77/48/c43b8a72b916cc70896aa431b0fc00d1481ae34e28dc55e2144f4c77916b/gym-0.17.1.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 351kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scipy (from gym)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "\u001b[K    33% |██████████▉                     | 8.8MB 380kB/s eta 0:00:46^C\n",
      "\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment you are going to:\n",
    "* implement DQN and doubleDQN\n",
    "* use them on CartPole, LunarLander and (Optionally) BreakOut environments\n",
    "\n",
    "### We use PyTorch for neural networks. If you are new to PyTorch see tutorials:\n",
    "https://pytorch.org/tutorials/ <br>\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a closer look at the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_video_string(infix):\n",
    "    video = io.open(\n",
    "        f'./gym-results/openaigym.video.{infix}.video000000.mp4',\n",
    "        'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    dec_str = encoded.decode('ascii')\n",
    "    src_tag = f'<source src=\"data:video/mp4;base64,{dec_str}\" type=\"video/mp4\" /></video>'\n",
    "    html = f'<video width=\"360\" height=\"auto\" alt=\"test\" controls>{src_tag}</video>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xCEEEEEEB\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root = Path(\"results\")\n",
    "log_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will embed a video of random policy playing CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Here's the definition of a function that implements training loop. We'll get through its elements soon.\n",
    "Although, note that we use a warmup techinque to fill the buffer in the beginning of a training procedure.\n",
    "\n",
    "The loop consists of:\n",
    "* Picking an action using the policy:\n",
    "$$a = \\text{argmax}_a Q(s, a)$$\n",
    "* Sending the action to environment (`step`)\n",
    "* Sampling N examples from the ReplayBuffer:\n",
    "$$ (s_i, a_i, s^\\prime_i, r_i)_{i=1,..,N} $$\n",
    "* Updating Q-network by minimizing the loss:\n",
    "$$y = r_i + \\gamma \\text{max}_a Q_{target}(s^{\\prime}_i, a) \\mathbb{1}\\{s^{\\prime}_i \\text{ is not terminal}\\} $$\n",
    "\n",
    "$$L_i = \\text{Loss}(Q(s_i, a_i), y)$$\n",
    "$$L = \\frac{1}{N} \\sum_i L_i \\rightarrow min,$$\n",
    "where $\\text{Loss}$ is $L_2$ loss or Huber loss\n",
    "* Updating target network every `target_update_every` iteration:\n",
    "$$\\theta^\\prime \\leftarrow \\theta \\tau + \\theta^\\prime (1 - \\tau)$$\n",
    "where $\\theta^\\prime$ - paremeters of $Q_{target}$ network, <br>\n",
    "$\\theta$ - paremeters of $Q$ network,<br>\n",
    "$\\tau \\in [0, 1]$\n",
    "\n",
    "Note, that we also use a logger that writes training info to `CSV` file, `TensorBoard` and `stdout`. We use `CSV` and `stdout` throughout the notebook, but you can use TB for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Implement ReplayBuffer from `lib/rollout.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    *,\n",
    "    log_dir,\n",
    "    prefix,\n",
    "    buffer_size,\n",
    "    n_steps,\n",
    "    warmup_steps,\n",
    "    target_update_every,\n",
    "    log_every,\n",
    "    save_every\n",
    "):\n",
    "    \"\"\"Training loop procedure.\n",
    "    \n",
    "    Arguments:\n",
    "        agent: DQN object.\n",
    "        env: Gym environment.\n",
    "        log_dir: PathLib path to logging dir.\n",
    "        prefix: Prefix for the experiment.\n",
    "        buffer_size: Maximum capacity of the replay buffer.\n",
    "        n_steps: Total number of training iterations.\n",
    "        warmup_steps: Number of first iterations that employ random policy.\n",
    "        target_update_every: Number of iterations between target network updates.\n",
    "        log_every: Log frequency\n",
    "        save_every: Save frequency\n",
    "        \n",
    "    Returns agent after training.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger = Logger(log_dir, prefix)\n",
    "    episode_reward = []\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    obs_cur = env.reset()\n",
    "    for i in range(n_steps + 1):\n",
    "        if i > warmup_steps:\n",
    "            obs = torch.FloatTensor(obs_cur).unsqueeze(0)\n",
    "            act = agent.pick_action(obs)\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "\n",
    "        obs_prev = obs_cur\n",
    "        obs_cur, rew, done, info = env.step(act)\n",
    "        episode_reward.append(rew)\n",
    "\n",
    "        memory.push(obs_prev, act, obs_cur, rew, done)\n",
    "\n",
    "        if done:\n",
    "            rew = np.sum(episode_reward)\n",
    "            episode_reward = []\n",
    "            logger.log_arr_kv(\"reward/reward\", rew)\n",
    "            obs_cur = env.reset()\n",
    "\n",
    "        if i > warmup_steps:\n",
    "\n",
    "            loss, q, q_est = agent.update_value(memory)\n",
    "            logger.log_arr_kv(\"loss/bellman_error_t\", loss)\n",
    "            logger.log_arr_kv(\"misc/q_t\", q)\n",
    "            logger.log_arr_kv(\"misc/q_est_t\", q_est)\n",
    "\n",
    "            if i % target_update_every == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            if i % log_every == 0:\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_mean\", np.mean)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_max\", np.max)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_min\", np.min)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_std\", np.std)\n",
    "                logger.reduce_arr_kv(\"loss/bellman_error_t\", \"loss/bellman_error\", np.mean)\n",
    "                logger.reduce_arr_kv(\"misc/q_t\", \"misc/q\", np.mean)\n",
    "                logger.reduce_arr_kv(\"misc/q_est_t\", \"misc/q_est\", np.mean)\n",
    "                logger.log_kv(\"misc/epsilon\", agent.eps)\n",
    "                logger.log_kv(\"misc/timestep\", i)\n",
    "                logger.write_logs(skip_arrs=True)\n",
    "\n",
    "            if i % save_every == 0:\n",
    "                num = i // save_every + 1\n",
    "                save_dir = log_dir / \"checkpoints\"\n",
    "                save_dir.mkdir(exist_ok=True)\n",
    "                agent.save_to(save_dir, prefix=hex(num)[2:].upper())\n",
    "\n",
    "        agent.update_eps()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "#### Task 1: Implement MLP network in `lib/network.py`\n",
    "#### Task 2: Implement DQN in `lib/agent.py`\n",
    "\n",
    "Now, run the next two cells. You should obtain the highest (500) reward in less than 100000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! You can try to use small `target_update_every`. E.g. value of 1 corresponds to fitted Q-iteration. The expected outcome is unstable or even divergent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=.2,\n",
    "    eps_decay=(.2 - .02) / 10**5,\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    discount_factor=.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"cartpole-v1\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=5 * 10**4,\n",
    "    n_steps=10**5,\n",
    "    warmup_steps=1000,\n",
    "    target_update_every=500,\n",
    "    log_every=500,\n",
    "    save_every=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our learned policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(data, name=None):\n",
    "    name = name or \"Mean reward\"\n",
    "    plt.plot(data['misc/timestep'], data['reward/reward_mean'], label=name)\n",
    "    lower = np.array(data['reward/reward_mean'] - data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    upper = np.array(data['reward/reward_mean'] + data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    examples = np.array(data['misc/timestep'])\n",
    "    plt.fill_between(list(examples), list(lower), list(upper), color='blue', alpha=0.1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots a mean reward against training iterations. Note, that the agent may sometimes degrade its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"cartpole-v1\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander\n",
    "\n",
    "Okay, now that we're done with very simple CartPole environment, let's move to something more complicated. \n",
    "\n",
    "LunarLander is a simulation game where a player has to control the capsule and land it on the zone marked with flags. If an agent achieves more than 200 scores, the environment is said to be solved.\n",
    "\n",
    "##### First, you should install Box2D support in Gym: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The random policy quickly fails the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Play a bit with hyperparameters. You have to gain some intuition about what each of them does. You have to achieve mean score of 200 in ~200 000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main drawbacks of vanila DQN is that it overestimates Q-values. More details could be found in the original paper. In a couple of words, we have to estimate an expecation of maximum, not a maximum of expectations as we do in vanila DQN. The trick is to use the second network for the updates (we use target network for this):\n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q(s^\\prime, a) ) \\mathbb{1}\\{s^{\\prime} \\text{ is not terminal}\\}$$\n",
    "\n",
    "Compared to \n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q_{target}(s^\\prime, a) ) \\mathbb{1}\\{s^{\\prime} \\text{ is not terminal}\\}$$\n",
    "\n",
    "from vanila DQN. Which is the same as:\n",
    "\n",
    "$$y = r + \\gamma \\text{max}_a Q_{target}(s^\\prime, a) \\mathbb{1}\\{s^{\\prime} \\text{ is not terminal}\\}$$\n",
    "\n",
    "Double Q-learning paper: https://papers.nips.cc/paper/3964-double-q-learning <br>\n",
    "Double DQN paper: https://arxiv.org/abs/1509.06461\n",
    "\n",
    "#### Task 4. Implement Double DQN in `lib/agent.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's try it for the LunarLander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_double_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should obtain better curve for Double DQN, however it is not always the case. For such environent the improvement could be neglectable. Although, you have to at least reach the same performance. \n",
    "### You may want to compare the performance on _CartPole_ as, unlike _LunarLander_ , its reward function is dense. However, it may be too simple to reveal any difference.\n",
    "### Thus, it is much better to compare DQN and Double DQN on Atari benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "df_double = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_double_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df, \"DQN\")\n",
    "plot_progress(df_double, \"Double DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the graphs of estimated Q values by DQN and Double DQN. Could you describe what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"estimated Q value\")\n",
    "plt.plot(df['misc/timestep'], df['misc/q_est'], label=\"DQN\")\n",
    "plt.plot(df_double['misc/timestep'], df_double['misc/q_est'], label=\"Double DQN\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI [Optional]\n",
    "\n",
    "In this section, you can try the algorithm on the larger problem from Atari suit. This section is optional. The training procedure is computationally cumbersome, so that ideally one may want to use GPU and powerful CPU for this task. Also note, that usually people choose larger `buffer_size` for this benchmark, which may consume anormous amount of RAM. To avoid the memory overflow, you may consider to use lossless compression algorithm to save a lot of space by compressing observations. See RLLib or Catalyst.RL for example.\n",
    "\n",
    "If you decided to go through the task, note, that typical choice of hyperparameters for Atari differs from previous ones. See RLLib's configs, for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone and install `baselines` lib. We'll use atari wrappers from it.\n",
    "\n",
    "Here are commands for that:\n",
    "```bash\n",
    "git clone https://github.com/openai/baselines\n",
    "cd baselines\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from lib.utils import TransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "\n",
    "    is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "        env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "\n",
    "    if is_atari:\n",
    "        max_epi_steps = 108000\n",
    "        if max_epi_steps < 0:\n",
    "            max_epi_steps = None\n",
    "\n",
    "        env = make_atari(name, max_epi_steps)\n",
    "        env = wrap_deepmind(env, True, True, True, True)\n",
    "        env = TransposeImage(env, op=[2, 0, 1])\n",
    "        \n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    act = env.action_space.sample()\n",
    "    if env.step(act)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", SEED)\n",
    "\n",
    "value_network = get_network(\"vision\")(\n",
    "    env.observation_space.shape[0],\n",
    "    256,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"breakout-v4\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=...,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=...,\n",
    "    save_every=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"breakout-v4\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
